{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 300, 2)\n",
      "(49,)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "\n",
    "\n",
    "def extract_epochs(signal, annotation):\n",
    "    epochs = None\n",
    "    labels = []\n",
    "    symbols = annotation.symbol\n",
    "    timepoints = annotation.sample\n",
    "    for i in range(len(timepoints)):\n",
    "        if i < 3:\n",
    "            continue\n",
    "        timepoint = timepoints[i]\n",
    "        if i == 0:\n",
    "            prev_timepoint = 0\n",
    "        else:\n",
    "            prev_timepoint = timepoints[i-1]\n",
    "#         print(\"prev_time point is {}, timepoint is{}\".format(prev_timepoint, timepoint))\n",
    "            \n",
    "        label = symbols[i-1]\n",
    "        \n",
    "        \n",
    "        if epochs is None:\n",
    "            epochs = signal[timepoint - 150: timepoint + 150, :]\n",
    "        elif epochs.shape[0] == 300:\n",
    "            epochs = np.stack((epochs, signal[timepoint - 150: timepoint + 150, :]))\n",
    "        else:\n",
    "            epochs = np.concatenate((epochs, signal[timepoint - 150: timepoint + 150, :].reshape(1, 300, 2)),axis = 0)\n",
    "\n",
    "        labels.append(label)\n",
    "    return epochs, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f = '100'\n",
    "filename = 'C:\\\\Users\\\\Kun\\\\ecg\\\\' + f\n",
    "record = wfdb.rdsamp(filename, sampto = 15000)\n",
    "signal, fields = wfdb.srdsamp(filename, sampfrom=800)\n",
    "annotation = wfdb.rdann(filename, 'atr', sampto = 15000)\n",
    "epochs, labels= extract_epochs(signal, annotation)\n",
    "epochs = np.array(epochs)\n",
    "labels = np.array(labels)\n",
    "print(epochs.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(4,1))\n",
    "# gs  = matplotlib.gridspec.GridSpec(4, 1)\n",
    "# ax0 = plt.subplot(gs[0])\n",
    "# ax1 = plt.subplot(gs[1])\n",
    "# ax2 = plt.subplot(gs[2])\n",
    "# ax3 = plt.subplot(gs[3])\n",
    "\n",
    "# fig.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "# ax1.plot(epochs[4])\n",
    "# ax2.plot(epochs[5])\n",
    "# ax3.plot(epochs[6])\n",
    "# ax0.plot(epochs[7])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8394, 300, 2)\n",
      "(8394,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = ['100','101','102','103','104','105', '106', '107','108', '109','111','112','113','114','115', '116','117','118','119', '121','122','123',\n",
    "         '124', '200','201','202','203','205','207','208','209',\n",
    "        '210','212','213','214','215','217','219','220','221',\n",
    "        '223','228','230','231','232','233','234']\n",
    "labels_total = []\n",
    "epochs_total = None\n",
    "\n",
    "for f in files:\n",
    "    filename = 'C:\\\\Users\\\\Kun\\\\ecg\\\\' + f\n",
    "\n",
    "    signal, fields = wfdb.srdsamp(filename, sampfrom = 0)\n",
    "    annotation = wfdb.rdann(filename, 'atr', sampto = 50000)\n",
    "    epochs, labels= extract_epochs(signal, annotation)\n",
    "    \n",
    "    if epochs_total is None:\n",
    "        epochs_total = epochs\n",
    "    else:\n",
    "        epochs_total = np.concatenate((epochs_total, epochs))\n",
    "    labels_total = labels_total + labels\n",
    "\n",
    "labels_total = np.array(labels_total)\n",
    "    \n",
    "print(epochs_total.shape)\n",
    "print(labels_total.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the occurance of ! is: 44\n",
      "the occurance of \" is: 27\n",
      "the occurance of + is: 90\n",
      "the occurance of / is: 522\n",
      "the occurance of A is: 114\n",
      "the occurance of F is: 51\n",
      "the occurance of L is: 568\n",
      "the occurance of N is: 5755\n",
      "the occurance of Q is: 3\n",
      "the occurance of R is: 553\n",
      "the occurance of V is: 532\n",
      "the occurance of [ is: 2\n",
      "the occurance of ] is: 2\n",
      "the occurance of a is: 4\n",
      "the occurance of e is: 1\n",
      "the occurance of f is: 69\n",
      "the occurance of x is: 2\n",
      "the occurance of | is: 18\n",
      "the occurance of ~ is: 37\n"
     ]
    }
   ],
   "source": [
    "def count_occurance(labels_total):\n",
    "#     labels_total = labels_total.tolist()\n",
    "    unique_label, occurance = np.unique(labels_total, return_counts = True)\n",
    "    for i in range(len(unique_label)):\n",
    "        print(\"the occurance of {} is: {}\".format(unique_label[i], occurance[i]))\n",
    "        \n",
    "count_occurance(labels_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the occurance of / is: 522\n",
      "the occurance of A is: 114\n",
      "the occurance of L is: 568\n",
      "the occurance of N is: 701\n",
      "the occurance of R is: 553\n",
      "the occurance of V is: 532\n"
     ]
    }
   ],
   "source": [
    "count_N = 0\n",
    "max_N = 700\n",
    "epochs_new = None\n",
    "labels_new = list()\n",
    "for index, epoch in enumerate(epochs_total):\n",
    "    if labels_total[index] not in ['/', 'A','L','N','R','V']:\n",
    "        continue\n",
    "    epoch = epoch.reshape(1, 300, 2)\n",
    "    if count_N > max_N and labels_total[index] == 'N':\n",
    "        continue\n",
    "    if labels_total[index] == 'N':\n",
    "        count_N = count_N +1\n",
    "\n",
    "    if epochs_new is None:\n",
    "        epochs_new = epoch\n",
    "    else:\n",
    "        epochs_new = np.concatenate((epochs_new, epoch))\n",
    "\n",
    "        \n",
    "    labels_new = labels_new + list(labels_total[index])\n",
    "\n",
    "count_occurance(labels_new)\n",
    "labels_new = np.array(labels_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2990, 300, 2)\n",
      "Classification accuracy: 0.422659 / Chance level: 0.765552\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          /       0.47      0.65      0.55       110\n",
      "          A       0.00      0.00      0.00        15\n",
      "          L       0.47      0.12      0.20       130\n",
      "          N       0.31      0.69      0.43       131\n",
      "          R       0.50      0.56      0.53       102\n",
      "          V       1.00      0.07      0.14       110\n",
      "\n",
      "avg / total       0.53      0.41      0.35       598\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kun\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sklearn # noqa\n",
    "from sklearn.model_selection import ShuffleSplit  # noqa\n",
    "from sklearn.svm import SVC  # noqa\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = sklearn.linear_model.LogisticRegression(C=1)\n",
    "\n",
    "y = labels_new\n",
    "X = epochs_new\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "# Define a monte-carlo cross-validation generator (reduce variance):\n",
    "cv = ShuffleSplit(n_splits=4, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "\n",
    "for train_idx, test_idx in cv.split(labels_new):\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    X_train = X[train_idx][:,0]\n",
    "    X_test = X[test_idx][:,0]\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # fit classifier\n",
    "    y_pred = clf.predict(X_test)\n",
    "    scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "# Printing the results\n",
    "class_balance = np.mean(labels == labels[0])\n",
    "class_balance = max(class_balance, 1. - class_balance)\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.185 -0.19  -0.195 -0.195 -0.19  -0.185 -0.185 -0.19  -0.2   -0.215]\n",
      "[-0.185 -0.005 -0.005  0.     0.005  0.005  0.    -0.005 -0.01  -0.015]\n"
     ]
    }
   ],
   "source": [
    "def take_diff(epochs):\n",
    "    temp = epochs.copy()\n",
    "    for i in range(epochs.shape[0]):\n",
    "        for j in range(epochs.shape[1]):\n",
    "            if j != 0:  \n",
    "                temp[i, j] = epochs[i,j] - epochs[i,j-1]\n",
    "    return temp\n",
    "\n",
    "\n",
    "epochs_diff = take_diff(epochs_new)\n",
    "print(epochs_new[0, :10, 1])      \n",
    "print(epochs_diff[0,:10, 1])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2990, 300, 2)\n",
      "Classification accuracy: 0.422659 / Chance level: 0.765552\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          /       0.47      0.65      0.55       110\n",
      "          A       0.00      0.00      0.00        15\n",
      "          L       0.47      0.12      0.20       130\n",
      "          N       0.31      0.69      0.43       131\n",
      "          R       0.50      0.56      0.53       102\n",
      "          V       1.00      0.07      0.14       110\n",
      "\n",
      "avg / total       0.53      0.41      0.35       598\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kun\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = sklearn.linear_model.LogisticRegression(C=1)\n",
    "\n",
    "y = labels_new\n",
    "X = epochs_diff\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "# Define a monte-carlo cross-validation generator (reduce variance):\n",
    "cv = ShuffleSplit(n_splits=4, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "\n",
    "for train_idx, test_idx in cv.split(labels_new):\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    X_train = X[train_idx][:,0]\n",
    "    X_test = X[test_idx][:,0]\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # fit classifier\n",
    "    y_pred = clf.predict(X_test)\n",
    "    scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "# Printing the results\n",
    "class_balance = np.mean(labels == labels[0])\n",
    "class_balance = max(class_balance, 1. - class_balance)\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "def convert_sign_into_num(labels):\n",
    "    unique = np.unique(labels)\n",
    "    num = np.arange(len(unique))\n",
    "    zipped = dict(zip(unique, num))\n",
    "    labels_num = []\n",
    "    for i in labels:\n",
    "        labels_num.append(zipped[i])\n",
    "    return labels_num\n",
    "\n",
    "labels_num = convert_sign_into_num(labels_new)\n",
    "print( len(np.unique(labels_new)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2392, 300, 2)\n",
      "(2392, 300, 2)\n",
      "Train on 2392 samples, validate on 598 samples\n",
      "Epoch 1/4\n",
      "2392/2392 [==============================] - 9s - loss: 1.0190 - acc: 0.6288 - val_loss: 0.4718 - val_acc: 0.8729\n",
      "Epoch 2/4\n",
      "2392/2392 [==============================] - 8s - loss: 0.6917 - acc: 0.7931 - val_loss: 0.4324 - val_acc: 0.8696\n",
      "Epoch 3/4\n",
      "2392/2392 [==============================] - 8s - loss: 2.3294 - acc: 0.6672 - val_loss: 1.7959 - val_acc: 0.2676\n",
      "Epoch 4/4\n",
      "2392/2392 [==============================] - 10s - loss: 1.7289 - acc: 0.2007 - val_loss: 1.7024 - val_acc: 0.1756\n",
      "Test loss: 1.70242164247\n",
      "Test accuracy: 0.17558528458\n",
      "(2392, 300, 2)\n",
      "(2392, 300, 2)\n",
      "Train on 2392 samples, validate on 598 samples\n",
      "Epoch 1/4\n",
      "2392/2392 [==============================] - 9s - loss: 1.1147 - acc: 0.6074 - val_loss: 0.8089 - val_acc: 0.7542\n",
      "Epoch 2/4\n",
      "2392/2392 [==============================] - 9s - loss: 0.7178 - acc: 0.7596 - val_loss: 0.4612 - val_acc: 0.8094\n",
      "Epoch 3/4\n",
      "2392/2392 [==============================] - 10s - loss: 0.5871 - acc: 0.8177 - val_loss: 0.3089 - val_acc: 0.9080\n",
      "Epoch 4/4\n",
      "2392/2392 [==============================] - 9s - loss: 0.5048 - acc: 0.8478 - val_loss: 0.3136 - val_acc: 0.9013\n",
      "Test loss: 0.313604884124\n",
      "Test accuracy: 0.901337792443\n",
      "(2392, 300, 2)\n",
      "(2392, 300, 2)\n",
      "Train on 2392 samples, validate on 598 samples\n",
      "Epoch 1/4\n",
      "2392/2392 [==============================] - 9s - loss: 0.9521 - acc: 0.6576 - val_loss: 0.6045 - val_acc: 0.8244\n",
      "Epoch 2/4\n",
      "2392/2392 [==============================] - 9s - loss: 0.5792 - acc: 0.8253 - val_loss: 0.4040 - val_acc: 0.8880\n",
      "Epoch 3/4\n",
      "2392/2392 [==============================] - 9s - loss: 0.5114 - acc: 0.8508 - val_loss: 0.4289 - val_acc: 0.8679\n",
      "Epoch 4/4\n",
      "2392/2392 [==============================] - 9s - loss: 0.5177 - acc: 0.8516 - val_loss: 0.3912 - val_acc: 0.8913\n",
      "Test loss: 0.391156680209\n",
      "Test accuracy: 0.891304348225\n",
      "(2392, 300, 2)\n",
      "(2392, 300, 2)\n",
      "Train on 2392 samples, validate on 598 samples\n",
      "Epoch 1/4\n",
      "2392/2392 [==============================] - 9s - loss: 1.0954 - acc: 0.6079 - val_loss: 0.6717 - val_acc: 0.7893\n",
      "Epoch 2/4\n",
      "2392/2392 [==============================] - 9s - loss: 0.8027 - acc: 0.7232 - val_loss: 0.6077 - val_acc: 0.8579\n",
      "Epoch 3/4\n",
      "2392/2392 [==============================] - 9s - loss: 0.5750 - acc: 0.8253 - val_loss: 0.4311 - val_acc: 0.8763\n",
      "Epoch 4/4\n",
      "2392/2392 [==============================] - 9s - loss: 0.4855 - acc: 0.8616 - val_loss: 0.3758 - val_acc: 0.8829\n",
      "Test loss: 0.375804608582\n",
      "Test accuracy: 0.882943144809\n",
      "Classification accuracy: 0.712793 \n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "cv = ShuffleSplit(n_splits=4, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "labels = np.array(labels_num)\n",
    "epochs_data = epochs_new\n",
    "\n",
    "\n",
    "\n",
    "for train_idx, test_idx in cv.split(labels):\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "\n",
    "\n",
    "    num_train = X_train.shape[0]\n",
    "    num_test = X_test.shape[0]\n",
    "    print(X_train.shape)\n",
    "    \n",
    "#     X_train = np.swapaxes(X_train, 1, 2)\n",
    "#     X_test = np.swapaxes(X_test, 1, 2)\n",
    "\n",
    "    print(X_train.shape)\n",
    "\n",
    "    num_classes = 6\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(8, kernel_size=(30),\n",
    "                     activation='relu',\n",
    "                     input_shape=(300,2)))\n",
    "    model.add(Conv1D(8, kernel_size=(30), activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=1,\n",
    "              epochs=4,\n",
    "              verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    scores.append(score[1])\n",
    "\n",
    "# Printing the results\n",
    "print(\"Classification accuracy: %f \" % (np.mean(scores)))\n",
    "\n",
    "#lit! after including only 6 classes, the acc went from 59 to 69!\n",
    "#lit! if we change batch size from 64 to 32, teh acc went up from 67 to 76!\n",
    "#lit! when we change the batch size to 8, the acc is 90%\n",
    "#when batch size is 4, the acc is 92%\n",
    "#when batch size is 8 and epochs is 10, the acc is 93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
